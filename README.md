# ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style Transfer

## Abstract

Chatbot-based customer support services have significantly advanced with the introduction of large language models (LLMs), enabling enhanced response quality and broader application across industries. However, while these advancements focus on reducing business costs and improving customer satisfaction, limited attention has been given to the experiences of customer service agents, who are critical to the service ecosystem. A major challenge faced by agents is the stress induced by verbal abuse and emotionally charged interactions, which not only impairs their efficiency but also negatively impacts customer satisfaction and business outcomes. In this work, we propose an LLM-based request-response transformer designed not to replace agents but to safeguard their mental well-being. Our approach leverages LLMs to mediate emotionally charged customer requests, ensuring high-quality responses while alleviating the psychological burden on agents. Furthermore, the application is implemented as a Chrome extension, making it highly adaptable and easy to integrate into existing systems. By addressing these challenges, our method aims to enhance the overall service experience for businesses, customers, and agents alike.

## Overview of the ProxyLLM

<img width="1040" alt="스크린샷 2024-12-09 오후 9 21 33" src="https://github.com/user-attachments/assets/665aabe3-c797-41f9-a4ac-f1a148970728">

## LLM API Server

The backend server is a [Flask](https://flask.palletsprojects.com/en/3.0.x/) server.

### Starting the server

Before starting the server, blahblah
